{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e78b015",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "import random \n",
    "import copy\n",
    "import numpy as np\n",
    "import pygmo as pg\n",
    "import pandas as pd\n",
    "#from scipy.io import arff\n",
    "import pickle\n",
    "from multiprocessing import Pool\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "#from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe733431",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequential_predict_rules_outcome(rules_bodies,rules_classes,data,bounderies,predictions) : \n",
    "    for rule_index,rule in enumerate(rules_bodies): \n",
    "        for data_index, data_item in enumerate(data): \n",
    "            predictions[rule_index,data_index] = rules_classes[rule_index]\n",
    "            for item_index,item in enumerate(rule) : \n",
    "                if (item[0] == item[1]) and item[0] == bounderies[item_index,0]: \n",
    "                    continue \n",
    "                if (data_item[item_index] > item[1]) or (data_item[item_index] < item[0]) : \n",
    "                    predictions[rule_index,data_index] = not(rules_classes[rule_index])\n",
    "                    break\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c47a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def is_pareto_efficient(costs, return_mask = False):\n",
    "    \"\"\"\n",
    "    Find the pareto-efficient points\n",
    "    :param costs: An (n_points, n_costs) array\n",
    "    :param return_mask: True to return a mask\n",
    "    :return: An array of indices of pareto-efficient points.\n",
    "        If return_mask is True, this will be an (n_points, ) boolean array\n",
    "        Otherwise it will be a (n_efficient_points, ) integer array of indices.\n",
    "    \"\"\"\n",
    "    is_efficient = np.arange(costs.shape[0])\n",
    "    n_points = costs.shape[0]\n",
    "    next_point_index = 0  # Next index in the is_efficient array to search for\n",
    "    while next_point_index<len(costs):\n",
    "        nondominated_point_mask = np.any(costs>=costs[next_point_index], axis=1)\n",
    "        nondominated_point_mask[next_point_index] = True\n",
    "        is_efficient = is_efficient[nondominated_point_mask]  # Remove dominated points\n",
    "        costs = costs[nondominated_point_mask]\n",
    "        next_point_index = np.sum(nondominated_point_mask[:next_point_index])+1\n",
    "    if return_mask:\n",
    "        is_efficient_mask = np.zeros(n_points, dtype = bool)\n",
    "        is_efficient_mask[is_efficient] = True\n",
    "        return is_efficient_mask\n",
    "    else:\n",
    "        return is_efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c762cc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def CUDA1D_predict_rules_outcome(rules_bodies,rules_classes,data,bounderies,predictions,confusion_matricies,): \n",
    "    tx = cuda.threadIdx.x\n",
    "    # Block id in a 1D grid\n",
    "    ty = cuda.blockIdx.x\n",
    "    # Block width, i.e. number of threads per block\n",
    "    bw = cuda.blockDim.x\n",
    "    # Compute flattened index inside the array\n",
    "    pos = tx + ty * bw\n",
    "    if pos < len(rules_bodies):  # Check array boundaries\n",
    "        rule = rules_bodies[pos]\n",
    "        for data_index, data_item in enumerate(data):\n",
    "            predictions[pos,data_index] = rules_classes[pos]\n",
    "            for item_index,item in enumerate(rule) : \n",
    "                if (item[0] == item[1]) and item[0] == bounderies[item_index,0]: \n",
    "                    continue \n",
    "                if (data_item[item_index] > item[1]) or (data_item[item_index] < item[0]) : \n",
    "                    predictions[pos,data_index] = not(rules_classes[pos])\n",
    "                    break\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c78f110",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def CUDA1D_compute_rules_confusion_matricies(rules_bodies,rules_classes,data,y,bounderies,predictions,objectives):\n",
    "    tx = cuda.threadIdx.x\n",
    "    # Block id in a 1D grid\n",
    "    ty = cuda.blockIdx.x\n",
    "    # Block width, i.e. number of threads per block\n",
    "    bw = cuda.blockDim.x\n",
    "    # Compute flattened index inside the array\n",
    "    pos = tx + ty * bw\n",
    "    TP = 0\n",
    "    TN = 0 \n",
    "    FP = 0 \n",
    "    FN = 0\n",
    "    if pos < len(rules_bodies):  # Check array boundaries\n",
    "        rule = rules_bodies[pos]\n",
    "        for data_index, data_item in enumerate(data):\n",
    "            predictions[pos,data_index] = rules_classes[pos]\n",
    "            for item_index,item in enumerate(rule) : \n",
    "                if (item[0] == item[1]) and item[0] == bounderies[item_index,0]: \n",
    "                    continue \n",
    "                if (data_item[item_index] > item[1]) or (data_item[item_index] < item[0]) : \n",
    "                    predictions[pos,data_index] = not(rules_classes[pos])\n",
    "                    break\n",
    "            if predictions[pos,data_index] == y[data_index] == True: \n",
    "                TP = TP + 1 \n",
    "                    \n",
    "            if predictions[pos,data_index] == y[data_index] == False : \n",
    "                TN = TN + 1 \n",
    "                \n",
    "            if (predictions[pos,data_index] == True) and (predictions[pos,data_index] != y[data_index]):\n",
    "                FP = FP + 1\n",
    "                                                                     \n",
    "            if predictions[pos,data_index] == False and predictions[pos,data_index] != y[data_index]: \n",
    "                FN =FN+ 1\n",
    "        objectives[pos,0] = TP*1.0/(TP + FN)\n",
    "        objectives[pos,1] = TN*1.0/(TN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc4046",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AbstractProblem: \n",
    "    def __init__(self,nb_objectives,nb_variables): \n",
    "        self.nb_objectives = nb_objectives\n",
    "        self.nb_variables = nb_variables \n",
    "  \n",
    "    def evaluate_solution(self,solution) : \n",
    "        pass\n",
    "\n",
    "    def new_solution(self): \n",
    "        pass\n",
    "\n",
    "    def new_velocity(self): \n",
    "        pass \n",
    "class DefectPrediction(AbstractProblem): \n",
    "\n",
    "    def __init__(self,data,outcome_name = 'bug'): \n",
    "        super().__init__(2,1)\n",
    "        self.data = data \n",
    "        self.feature_description = {}\n",
    "        self.outcome_variable = outcome_name\n",
    "        self.y = np.array(data[self.outcome_variable].astype(bool))\n",
    "        self.X = data.drop(columns = [self.outcome_variable])\n",
    "        self.X_as_np = self.X.to_numpy(dtype = 'float64')\n",
    "        self.bounderies = np.zeros((len(self.X.columns),2),dtype = 'float64')\n",
    "        for index,column in enumerate(data.columns): \n",
    "            if column == outcome_name: \n",
    "                continue\n",
    "            self.feature_description[column] = {'type': 'numerical', 'min' : min(data[column]), 'max': max(data[column])}\n",
    "            self.bounderies[index] = np.array([min(data[column]),max(data[column])])\n",
    "        \n",
    "        self.features_description_as_list = list(self.feature_description.items())\n",
    "        self.rule_encoding = RuleEncoding(self.feature_description,empty_prob=0.1)\n",
    "\n",
    "    def evaluate_solution(self,solution): \n",
    "        rule = solution.variables[0]\n",
    "        y_prediction = []\n",
    "        for index,row in self.X.iterrows():\n",
    "            y_prediction.append(rule.evaluate_rule(row))\n",
    "\n",
    "        tp,fp,tn,fn = perf_measure(self.y,y_prediction)\n",
    "        solution.objectives = [-1*tp*1.0/(tp + fn),-1*tn*1.0/(tn + fp)]\n",
    "\n",
    "    def new_solution(self): \n",
    "        new_solution = Solution(1,2)\n",
    "        new_solution.variables[0] = RuleVariable(self.rule_encoding)\n",
    "        new_solution.variables[0].initialize()\n",
    "        return new_solution\n",
    "        \n",
    "    def new_velocity(self): \n",
    "        return self.rule_encoding.randomize_velocities()\n",
    "      \n",
    "class MOPSO: \n",
    "    def __init__(self,configuration,optimization_problem) : \n",
    "        self.optimization_problem = optimization_problem\n",
    "        self.default_config = {\n",
    "            'particles_per_class': 500,\n",
    "            'generation_number': 100,\n",
    "            'c1': 1,\n",
    "            'c2':1,\n",
    "            'empty_prob': 0.1,\n",
    "            'w': [0,0.8],\n",
    "            'ph1': [0,4],\n",
    "            'ph2': [0,4],\n",
    "        }\n",
    "        self.current_config = copy.deepcopy(self.default_config)\n",
    "        self.repository = None \n",
    "    \n",
    "        for parameter_name,value in configuration: \n",
    "            self.current_config[parameter_name] = value \n",
    "    \n",
    "    def evolve(self):\n",
    "        #initialization  \n",
    "        current_population = self.create_initial_population()\n",
    "        current_velocities = self.initialize_velocities()\n",
    "        \n",
    "        #evaluate solutions\n",
    "        for solution in current_population: \n",
    "            self.optimization_problem.evaluate_solution(solution)\n",
    "        current_best_positions = copy.deepcopy(current_population)\n",
    "\n",
    "        ndf, dl, dc, ndr = non_dominated_sorting(current_population) \n",
    "        self.repository = [current_population[index] for index in ndf[0]]\n",
    "        leader_assignment = self.assign_local_leader(current_population)\n",
    "\n",
    "        for generation_count in range(self.current_config['generation_number']) : \n",
    "            if (generation_count % 20 == 0):\n",
    "                print('generation:',generation_count)\n",
    "            current_velocities = self.update_velocities(current_velocities,current_population,current_best_positions,leader_assignment)\n",
    "            current_population = self.update_position(current_population,current_velocities) \n",
    "            for index,solution in enumerate(current_population):\n",
    "                self.optimization_problem.evaluate_solution(solution)\n",
    "                best_position_update = [solution,current_best_positions[index]]\n",
    "                ndf, dl, dc, ndr = non_dominated_sorting(best_position_update)                \n",
    "                current_best_positions[index] = random.choice([best_position_update[idx] for idx in ndf[0]])\n",
    "            ndf, dl, dc, ndr = non_dominated_sorting(current_population) \n",
    "            self.repository = [current_population[index] for index in ndf[0]]\n",
    "            leader_assignment = self.assign_local_leader(current_population)\n",
    "        \n",
    "\n",
    "    def update_position(self,current_position,new_velocities) : \n",
    "        new_positions = []\n",
    "        for position_index, position in enumerate(current_position) : \n",
    "            new_position = copy.deepcopy(position)\n",
    "            for feature_name,feature_velocity in new_velocities[position_index].items():\n",
    "                #if  not (new_position.variables[0].rule_body[feature_name][0] ==  self.optimization_problem.rule_encoding.features_description[feature_name]['min'] and new_position.variables[0].rule_body[feature_name][1] ==  self.optimization_problem.rule_encoding.features_description[feature_name]['min']):\n",
    "                new_position.variables[0].rule_body[feature_name] = self.optimization_problem.rule_encoding.mod_operator(feature_name,position.variables[0].rule_body[feature_name],feature_velocity)\n",
    "            new_positions.append(new_position)\n",
    "        \n",
    "        return new_positions \n",
    "\n",
    "    def update_velocities(self,current_velocities,current_population,current_best_positions,leader_assignment) : \n",
    "        new_velocities = []\n",
    "        for index,velocity in enumerate(current_velocities): \n",
    "            new_velocity = copy.deepcopy(velocity)\n",
    "            for feature_name,value in new_velocity.items(): \n",
    "                omega = random.uniform(self.current_config['w'][0],self.current_config['w'][1])\n",
    "                ph1 = random.uniform(self.current_config['ph1'][0],self.current_config['ph1'][1])\n",
    "                ph2 = random.uniform(self.current_config['ph2'][0],self.current_config['ph2'][1])\n",
    "                new_velocity[feature_name] = new_velocity[feature_name]*omega \\\n",
    "                +self.current_config['c1']*ph1*(current_best_positions[index].variables[0].rule_body[feature_name] - current_population[index].variables[0].rule_body[feature_name])\\\n",
    "                +self.current_config['c2']*ph2*(self.repository[leader_assignment[index]].variables[0].rule_body[feature_name] - current_population[index].variables[0].rule_body[feature_name])\n",
    "            new_velocities.append(new_velocity)\n",
    "        return new_velocities\n",
    "\n",
    "    def create_initial_population(self): \n",
    "        new_population = []\n",
    "        for rule_class in [True,False]: \n",
    "            for _ in range(self.current_config['particles_per_class']): \n",
    "                new_solution = self.optimization_problem.new_solution()\n",
    "                new_solution.variables[0].set_class(rule_class)\n",
    "                new_population.append(new_solution)\n",
    "        \n",
    "        return new_population\n",
    "\n",
    "    def assign_local_leader(self,population) : \n",
    "        leader_assignment = [None]*len(population)\n",
    "        for index,solution in enumerate(population): \n",
    "            best_sigma_dist = 99999999999999999999999999\n",
    "            for repository_solution_index,no_dominated_solution in enumerate(self.repository): \n",
    "                if abs((no_dominated_solution.get_sigma(0,1) -  solution.get_sigma(0,1)))< best_sigma_dist: \n",
    "                    best_sigma_dist = abs((no_dominated_solution.get_sigma(0,1) -  solution.get_sigma(0,1)))\n",
    "                    leader_assignment[index] = repository_solution_index\n",
    "        return leader_assignment\n",
    "\n",
    "\n",
    "    def initialize_velocities(self):\n",
    "        velocities = []\n",
    "        for _ in range(2*self.current_config['particles_per_class']): \n",
    "            velocities.append(self.optimization_problem.new_velocity())\n",
    "        return velocities\n",
    "    \n",
    "\n",
    "class RuleVariable: \n",
    "    def __init__(self,encoding) : \n",
    "        self.rule_encoding = encoding\n",
    "        self.rule_body = None \n",
    "    \n",
    "    def initialize(self):\n",
    "        self.rule_body = self.rule_encoding.randomize()\n",
    "    \n",
    "    def set_class(self,new_class): \n",
    "        self.rule_class = new_class\n",
    "\n",
    "    def evaluate_rule(self,env) : \n",
    "      for feature_name, feature_value in self.rule_body.items(): \n",
    "          env_feature_value = env[feature_name]\n",
    "          if self.rule_encoding.features_description[feature_name]['type'] ==  'CATEGORICAL':\n",
    "              if env_feature_value == -1 : \n",
    "                  continue\n",
    "              else: \n",
    "                  if feature_value != env_feature_value :\n",
    "                      return not(self.rule_class) \n",
    "          #feature is numerical\n",
    "          else: \n",
    "              if feature_value[0] == feature_value[1] and feature_value[0] == self.rule_encoding.features_description[feature_name]['min']:\n",
    "                  continue\n",
    "              if env_feature_value < feature_value[0] or (env_feature_value > feature_value[1]) :\n",
    "                  return not(self.rule_class)  \n",
    "      return self.rule_class\n",
    "\n",
    "class Solution: \n",
    "    def __init__(self,nb_variables,nb_objectives):\n",
    "        self.nb_variables = nb_variables\n",
    "        self.nb_objectives = nb_objectives \n",
    "        self.objectives = [None]*nb_objectives\n",
    "        self.variables = [None]*nb_variables\n",
    "        self.attributes = {}\n",
    "    \n",
    "    def get_sigma(self,obj1,obj2): \n",
    "        return (self.objectives[obj1]**2 - self.objectives[obj2]**2)*1.0 / (self.objectives[obj1]**2 + self.objectives[obj2]**2)\n",
    "class RuleEncoding: \n",
    "  def __init__(self,features_description,empty_prob): \n",
    "      self.features_description = features_description\n",
    "      self.empty_prob = empty_prob\n",
    "  \n",
    "  def mod_operator(self,feature_name,values,velocity) : \n",
    "      update = values + velocity\n",
    "      max_overflow = None \n",
    "      min_overflow = None \n",
    "      if update[1] > self.features_description[feature_name]['max'] : \n",
    "          #print('overflow! max')\n",
    "          max_overflow = update[1] - self.features_description[feature_name]['max'] \n",
    "      if update[0] < self.features_description[feature_name]['min'] : \n",
    "          #print('overflow! min')\n",
    "          min_overflow = self.features_description[feature_name]['max'] - update[0]\n",
    "      if not (max_overflow is None) and not (min_overflow is None) :\n",
    "          #print('total overflow') \n",
    "          return (np.array([self.features_description[feature_name]['min'],self.features_description[feature_name]['min']])) \n",
    "      if not(max_overflow is None) : \n",
    "          update[1] = self.features_description[feature_name]['min'] +  max_overflow\n",
    "      if not(min_overflow is None) : \n",
    "          update[0] = self.features_description[feature_name]['max'] -  min_overflow\n",
    "      update.sort()\n",
    "      return update\n",
    "  \n",
    "  def get_features_number(self): \n",
    "      return len(self.features_description)\n",
    "\n",
    "  def randomize(self): \n",
    "    new_rule = {} \n",
    "    for feature_name,feature_description in self.features_description.items() :         \n",
    "        if feature_description['type'] == 'CATEGORICAL':\n",
    "            empty_rule_update = np.array([-1])\n",
    "            rule_update = np.array([random.choice([feature_description['values']])])\n",
    "        else: \n",
    "            empty_rule_update = np.array([feature_description['min'],feature_description['min']])\n",
    "            rule_update = np.sort(np.array([random.uniform(feature_description['min'], feature_description['max']),random.uniform(feature_description['min'], feature_description['max'])]))\n",
    "\n",
    "        if random.random() <= self.empty_prob: \n",
    "            new_rule[feature_name] = empty_rule_update\n",
    "        else :\n",
    "            new_rule[feature_name] = rule_update\n",
    "    return new_rule\n",
    "\n",
    "  def randomize_rule_body_np(self): \n",
    "      new_rule = np.zeros((self.get_features_number(),2),dtype='float32') \n",
    "      for index,(feature_name,feature_description) in enumerate(self.features_description.items()) :         \n",
    "          if feature_description['type'] == 'CATEGORICAL':\n",
    "              empty_rule_update = np.array([-1,-1])\n",
    "              rule_update = np.array([random.choice([feature_description['values']]),-1])\n",
    "          else: \n",
    "              empty_rule_update = np.array([feature_description['min'],feature_description['min']])\n",
    "              rule_update = np.sort(np.array([random.uniform(feature_description['min'], feature_description['max']),random.uniform(feature_description['min'], feature_description['max'])]))\n",
    "          if random.random() <= self.empty_prob: \n",
    "              new_rule[index,:] = empty_rule_update\n",
    "          else :\n",
    "              new_rule[index,:] = rule_update\n",
    "      return new_rule\n",
    "\n",
    "  def randomize_velocities(self): \n",
    "      new_velocity = {} \n",
    "      for feature_name,feature_description in self.features_description.items() :  \n",
    "          if feature_description['type'] == 'CATEGORICAL':\n",
    "              rule_update = np.array([random.choice([feature_description['values']])])\n",
    "          else: \n",
    "              rule_update = np.array([random.random(),random.random()])\n",
    "\n",
    "          new_velocity[feature_name] = rule_update\n",
    "      return new_velocity\n",
    "  \n",
    "  def randomize_velocity_np(self):\n",
    "      new_velocity = np.zeros(self.get_features_number(),2) \n",
    "      for index,(feature_name,feature_description) in self.features_description.items() :  \n",
    "          if feature_description['type'] == 'CATEGORICAL':\n",
    "              rule_update = np.array([random.choice([feature_description['values']])])\n",
    "          else: \n",
    "              rule_update = np.array([random.random(),random.random()])\n",
    "\n",
    "          new_velocity[index,:] = rule_update\n",
    "      return new_velocity\n",
    "       \n",
    "class GPUMOPSO(MOPSO): \n",
    "\n",
    "    def evolve(self):\n",
    "        rules_bodies,rules_classes = self.create_new_population()\n",
    "        current_velocities = self.initialize_velocities()\n",
    "        #evaluate solutions\n",
    "        objectives = self.evaluate_rules(rules_bodies,rules_classes)\n",
    "        current_best_positions = np.copy(rules_bodies)\n",
    "        current_best_positions_objectives =  np.copy(objectives)\n",
    "        ndf, dl, dc, ndr = pg.fast_non_dominated_sorting(objectives)\n",
    "        self.repository = ndf[0]\n",
    "        leader_assignment = self.assign_local_leader(objectives)\n",
    "\n",
    "        for generation_count in range(self.current_config['generation_number']) : \n",
    "            if (generation_count % 20 == 0):\n",
    "                print('generation:',generation_count)\n",
    "            current_velocities = self.update_velocities(current_velocities,rules_bodies,current_best_positions,leader_assignment)\n",
    "            rules_bodies = self.update_position(rules_bodies,current_velocities)\n",
    "            objectives = self.evaluate_rules(rules_bodies,rules_classes)\n",
    "            for index,solution in enumerate(rules_bodies):\n",
    "                best_position_update = [solution,current_best_positions[index]]\n",
    "                best_position_objective_update = np.array([objectives[index],current_best_positions_objectives[index]])\n",
    "                ndf, dl, dc, ndr = pg.fast_non_dominated_sorting(best_position_objective_update) \n",
    "                new_best_position_choice = random.choice(ndf[0])\n",
    "                current_best_positions[index] = best_position_update[new_best_position_choice]\n",
    "                current_best_positions_objectives[index] = best_position_objective_update[new_best_position_choice]\n",
    "            ndf, dl, dc, ndr = pg.fast_non_dominated_sorting(objectives)\n",
    "            self.repository = ndf[0] \n",
    "            leader_assignment = self.assign_local_leader(objectives)\n",
    "        for index in self.repository: \n",
    "            print(objectives[index])\n",
    "        return {\n",
    "            'rules':rules_bodies,\n",
    "            'rules_classes':rules_classes,\n",
    "            'objectives':objectives,\n",
    "            'no_dominated_rules_indicies': self.repository\n",
    "        }\n",
    "    def evaluate_rules(self,rules_bodies,rules_classes) :\n",
    "        predictions = np.empty(shape = (len(rules_bodies),len(self.optimization_problem.X)),dtype='bool')\n",
    "        objectives = np.empty(shape = (len(rules_bodies),2),dtype='float32')\n",
    "        threadsperblock = 8\n",
    "        blockspergrid = (len(predictions) + (threadsperblock - 1)) // threadsperblock\n",
    "        \n",
    "        rules_bodies_cuda = cuda.to_device(rules_bodies)\n",
    "        rules_classes_cuda =  cuda.to_device(rules_classes)\n",
    "        data_cuda = cuda.to_device(self.optimization_problem.X_as_np)\n",
    "        y_cuda = cuda.to_device(self.optimization_problem.y)\n",
    "        bounderies_cuda = cuda.to_device(self.optimization_problem.bounderies)\n",
    "        predictions_cuda = cuda.to_device(predictions)\n",
    "        objectives_cuda = cuda.to_device(objectives)\n",
    "        \n",
    "        #CUDA1D_compute_rules_confusion_matricies(rules_bodies,rules_classes,data,y,bounderies,predictions,objectives)\n",
    "        CUDA1D_compute_rules_confusion_matricies[blockspergrid, threadsperblock](rules_bodies_cuda,rules_classes_cuda,data_cuda,\n",
    "                                                                                 y_cuda,bounderies_cuda,predictions_cuda,objectives_cuda) \n",
    "        objectives = objectives_cuda.copy_to_host()\n",
    "        \n",
    "        #sequential_predict_rules_outcome(rules_bodies,rules_classes,self.optimization_problem.X_as_np,self.optimization_problem.bounderies,predictions) \n",
    "        return objectives*-1\n",
    "    \n",
    "    def assign_local_leader(self,objectives):\n",
    "        sigmas = self.get_sigmas(objectives)\n",
    "        leader_assignment = []\n",
    "        for sigma in sigmas : \n",
    "            least_sigma_dist = np.inf\n",
    "            leader_index = -1\n",
    "            for no_dominated_solution_index in self.repository: \n",
    "                new_sigma_dist = abs(sigma - sigmas[no_dominated_solution_index])\n",
    "                if ( new_sigma_dist < least_sigma_dist ): \n",
    "                    least_sigma_dist = new_sigma_dist\n",
    "                    leader_index = no_dominated_solution_index\n",
    "            leader_assignment.append(leader_index)\n",
    "        \n",
    "        return leader_assignment \n",
    "\n",
    "    def initialize_velocities(self):\n",
    "        return np.random.rand(self.current_config['particles_per_class']*2,self.optimization_problem.rule_encoding.get_features_number(),2)\n",
    "    \n",
    "    def create_new_population(self): \n",
    "        index = 0 \n",
    "        rules_bodies = np.zeros((self.current_config['particles_per_class']*2,self.optimization_problem.rule_encoding.get_features_number(),2),dtype='float64')\n",
    "        rules_classes = []\n",
    "        for rule_class in [True,False]:\n",
    "            for _ in range(self.current_config['particles_per_class']): \n",
    "                rules_bodies[index,:,:] = self.optimization_problem.rule_encoding.randomize_rule_body_np()\n",
    "                rules_classes.append(rule_class)\n",
    "                index += 1 \n",
    "        \n",
    "        return rules_bodies, np.array(rules_classes,dtype = 'bool')\n",
    "    \n",
    "    def predict_rules_outcome(self,rules_bodies,rules_classes,data) : \n",
    "        predictions = np.empty((len(rules_classes),len(data)), dtype=bool)\n",
    "        for rule_index,rule in enumerate(rules_bodies): \n",
    "            for data_index, data_item in enumerate(data): \n",
    "                for item_index,item in enumerate(rule) : \n",
    "                    if (item[0] == item[1]) and item[0] == self.optimization_problem.features_description_as_list[index]['min']: \n",
    "                        continue \n",
    "                    if (data_item[item_index] > item[1]) or (data_item[item_index] < item[0]) : \n",
    "                        predictions[rule_index,data_index] = not(rules_classes[rule_index])\n",
    "                        break\n",
    "                predictions[rule_index,data_index] = rules_classes[rule_index]\n",
    "\n",
    "        return predictions \n",
    "\n",
    "    def update_velocities(self,current_velocities,rules_bodies,current_best_positions,leader_assignment) :\n",
    "        for index in range(len(current_velocities)): \n",
    "            omega = np.random.uniform(low=self.current_config['w'][0],high=self.current_config['w'][1],size=(rules_bodies.shape[1],rules_bodies.shape[2]))\n",
    "            ph1 = np.random.uniform(low=self.current_config['ph1'][0],high=self.current_config['ph1'][1],size=(rules_bodies.shape[1],rules_bodies.shape[2]))\n",
    "            ph2 = np.random.uniform(low=self.current_config['ph2'][0],high=self.current_config['ph2'][1],size=(rules_bodies.shape[1],rules_bodies.shape[2]))\n",
    "            current_velocities[index] = current_velocities[index]*omega \\\n",
    "                +self.current_config['c1']*ph1*(current_best_positions[index] - rules_bodies[index])\\\n",
    "                +self.current_config['c2']*ph2*(rules_bodies[leader_assignment[index]] - rules_bodies[index])\n",
    "        return current_velocities\n",
    "    \n",
    "    def update_position(self,current_position,new_velocities) :\n",
    "        for position_index, position in enumerate(current_position) : \n",
    "            for velocity_item_index,velocity_item in enumerate(new_velocities[position_index]):\n",
    "                position[velocity_item_index] = self.mod_operator(self.optimization_problem.features_description_as_list[velocity_item_index][1],position[velocity_item_index],velocity_item)\n",
    "        return current_position\n",
    "         \n",
    "    \n",
    "    def mod_operator(self,feature_limits,values,velocity) : \n",
    "        update = values + velocity\n",
    "        #eps = 0.01\n",
    "        max_overflow = None \n",
    "        min_overflow = None \n",
    "        if update[1] > feature_limits['max'] : \n",
    "            max_overflow = update[1] - feature_limits['max'] \n",
    "\n",
    "        if update[0] < feature_limits['min'] : \n",
    "            min_overflow = feature_limits['min'] - update[0]\n",
    "      \n",
    "        if not (max_overflow is None) and not (min_overflow is None) :\n",
    "            return np.array([feature_limits['min'],feature_limits['min']])\n",
    "        if not(max_overflow is None) : \n",
    "            update[1] = feature_limits['min'] +  max_overflow\n",
    "        if not(min_overflow is None) : \n",
    "            update[0] = feature_limits['max'] -  min_overflow\n",
    "        update.sort()\n",
    "        #if np.any(update > feature_limits['max'] ) or np.any(update < feature_limits['min'] ): \n",
    "         #   velocity = np.array([random.random(),random.random()])\n",
    "          #  return np.array([feature_limits['min'] + (feature_limits['max'] - feature_limits['min'])*eps,feature_limits['max'] - (feature_limits['max'] - feature_limits['min'])*eps]).sort()\n",
    "        return update\n",
    "    \n",
    "    def get_sigmas(self,sol_objectives) : \n",
    "        first_component_square = (sol_objectives[:,0]**2)*1.0\n",
    "        second_component_square = (sol_objectives[:,1]**2)*1.0\n",
    "        return (first_component_square - second_component_square)/(first_component_square + second_component_square)\n",
    "    \n",
    "    def compute_objectives(self,predictions) : \n",
    "        objectives = np.empty((len(predictions),2))\n",
    "        for index,prediction in enumerate(predictions): \n",
    "            tn, fp, fn, tp = confusion_matrix(self.optimization_problem.y, prediction).ravel()\n",
    "            objectives[index,:] = np.array([tp*1.0/(tp + fn),tn*1.0 / (tn + fp)])\n",
    "        \n",
    "        return objectives\n",
    "    \n",
    "class HybridGPUMOPSO(MOPSO) :\n",
    "    \n",
    "    def evolve(self):\n",
    "        #initialization  \n",
    "        current_population = self.create_initial_population()\n",
    "        current_velocities = self.initialize_velocities()\n",
    "        #evaluate solutions\n",
    "        self.evaluate_rules(current_population)\n",
    "        current_best_positions = copy.deepcopy(current_population)\n",
    "        ndf = is_pareto_efficient(np.array([sol.objectives for sol in current_population]))\n",
    "        self.repository = [current_population[index] for index in ndf]\n",
    "        leader_assignment = self.assign_local_leader(current_population)\n",
    "\n",
    "        for generation_count in range(self.current_config['generation_number']) : \n",
    "            if (generation_count % 20 == 0):\n",
    "                print('generation:',generation_count)\n",
    "            current_velocities = self.update_velocities(current_velocities,current_population,current_best_positions,leader_assignment)\n",
    "            current_population = self.update_position(current_population,current_velocities) \n",
    "            self.evaluate_rules(current_population)\n",
    "            for index,solution in enumerate(current_population):\n",
    "                best_position_update = [solution,current_best_positions[index]]\n",
    "                ndf= is_pareto_efficient(np.array([sol.objectives for sol in best_position_update]))                \n",
    "                current_best_positions[index] = random.choice([best_position_update[idx] for idx in ndf])\n",
    "            ndf = is_pareto_efficient(np.array([sol.objectives for sol in current_population])) \n",
    "            self.repository = [current_population[index] for index in ndf]\n",
    "            leader_assignment = self.assign_local_leader(current_population)\n",
    "        for sol in self.repository: \n",
    "            print(sol.objectives)\n",
    "            \n",
    "    def evaluate_rules(self,solutions) :\n",
    "        \n",
    "        rules_bodies = np.array([[value for value in sol.variables[0].rule_body.values()] for sol in solutions])\n",
    "        rules_classes = np.array([sol.variables[0].rule_class for sol in solutions])\n",
    "        \n",
    "        predictions = np.empty(shape = (len(rules_bodies),len(self.optimization_problem.X)),dtype='bool')\n",
    "        objectives = np.empty(shape = (len(rules_bodies),2),dtype='float32')\n",
    "        threadsperblock = 8\n",
    "        blockspergrid = (len(predictions) + (threadsperblock - 1)) // threadsperblock\n",
    "        \n",
    "        rules_bodies_cuda = cuda.to_device(rules_bodies)\n",
    "        rules_classes_cuda =  cuda.to_device(rules_classes)\n",
    "        data_cuda = cuda.to_device(self.optimization_problem.X_as_np)\n",
    "        y_cuda = cuda.to_device(self.optimization_problem.y)\n",
    "        bounderies_cuda = cuda.to_device(self.optimization_problem.bounderies)\n",
    "        predictions_cuda = cuda.to_device(predictions)\n",
    "        objectives_cuda = cuda.to_device(objectives)\n",
    "        \n",
    "        #CUDA1D_compute_rules_confusion_matricies(rules_bodies,rules_classes,data,y,bounderies,predictions,objectives)\n",
    "        CUDA1D_compute_rules_confusion_matricies[blockspergrid, threadsperblock](rules_bodies_cuda,rules_classes_cuda,data_cuda,\n",
    "                                                                                 y_cuda,bounderies_cuda,predictions_cuda,objectives_cuda) \n",
    "        objectives = objectives_cuda.copy_to_host()\n",
    "        \n",
    "        #sequential_predict_rules_outcome(rules_bodies,rules_classes,self.optimization_problem.X_as_np,self.optimization_problem.bounderies,predictions) \n",
    "        for i,sol in enumerate(solutions): \n",
    "            sol.objectives = objectives[i,:]\n",
    "        return objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fee6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main \n",
    "#globals\n",
    "projects = {\"ambros\" : [\"mylyn\",\"pde\"],\"eclipse\" : [\"eclipse\"], \"ck\" : ['ant','velocity',\"camel\",\"poi\",\"prop\",\"synapse\",\"xalan\",\"xerces\",\"lucene\"]}\n",
    "projects_features = {\"ambros\" : [\"numberOfVersionsUntil:\",\"numberOfFixesUntil:\",\"numberOfRefactoringsUntil:\",\"numberOfAuthorsUntil:\",\"linesAddedUntil:\",\"maxLinesAddedUntil:\",\"avgLinesAddedUntil:\",\"linesRemovedUntil:\",\"maxLinesRemovedUntil:\",\"avgLinesRemovedUntil:\",\"codeChurnUntil:\",\"maxCodeChurnUntil:\",\"avgCodeChurnUntil:\",\"ageWithRespectTo:\",\"weightedAgeWithRespectTo:\"],\n",
    "            \"ck\" : [\"wmc\",\"dit\",\"noc\",\"cbo\",\"rfc\",\"lcom\",\"ca\",\"ce\",\"npm\",\"lcom3\",\"loc\",\"dam\",\"moa\",\"mfa\",\"cam\",\"ic\",\"cbm\",\"amc\",\"max_cc\",\"avg_cc\"],\n",
    "            \"eclipse\" : [\"pre\",\"ACD\",\"FOUT_avg\",\"FOUT_max\",\"FOUT_sum\",\"MLOC_avg\",\"MLOC_max\",\"MLOC_sum\",\"NBD_avg\",\"NBD_max\",\"NBD_sum\",\"NOF_avg\",\"NOF_max\",\"NOF_sum\",\"NOI\",\"NOM_avg\",\"NOM_max\",\"NOM_sum\",\"NOT\",\"NSF_avg\",\"NSF_max\",\"NSF_sum\",\"NSM_avg\",\"NSM_max\",\"NSM_sum\",\"PAR_avg\",\"PAR_max\",\"PAR_sum\",\"TLOC\",\"VG_avg\",\"VG_max\",\"VG_sum\"]\n",
    "            }\n",
    "outcome =  {\"ck\" : \"bug\",\"ambros\" : \"bugs\",\"eclipse\" : \"post\"}  \n",
    "K = 2 \n",
    "DATA_PATH = \"C:/Users/Motaz/Desktop/work/TSE_R3/MOCRDP/data/CV_data/eclipse\"\n",
    "RESULTS_PATH = './MOPSO results_final_CRDP/results_pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c22858",
   "metadata": {},
   "outputs": [],
   "source": [
    "#main \n",
    "n_runs = 100\n",
    "os.makedirs(RESULTS_PATH,exist_ok=True)\n",
    "for file_name in os.listdir(DATA_PATH) :\n",
    "    for i in range(n_runs):\n",
    "        if not ('train' in file_name) or not ('.csv' in file_name) :\n",
    "            continue \n",
    "\n",
    "        file_id = file_name.replace('.csv','')\n",
    "        if os.path.exists(os.path.join(RESULTS_PATH,file_name.replace('.csv','.pkl'))) :\n",
    "            print(file_name, ' already trained')\n",
    "            continue \n",
    "        \n",
    "        train_data = pd.read_csv(os.path.join(DATA_PATH,file_name))\n",
    "        \n",
    "        project_name = file_id.split(\"_\")[0]\n",
    "        for project in projects : \n",
    "          for pnames in projects[project] : \n",
    "            if pnames in project_name :\n",
    "              project_id = project \n",
    "              break\n",
    "        \n",
    "        \n",
    "        print('learning for '+ file_name + \" started\")\n",
    "        features = projects_features[project_id]\n",
    "        output_variable = outcome[project_id]\n",
    "        sdp_problem = DefectPrediction(train_data[features + [output_variable]],outcome_name=output_variable)\n",
    "        MOPSO_instance = GPUMOPSO(configuration = {},optimization_problem=sdp_problem) \n",
    "        run_results = MOPSO_instance.evolve()\n",
    "        print('learning for '+ file_name +  ' done')\n",
    "        final_file_name = file_name.replace(\".csv\",\"\") + f'-run{i}.pkl'\n",
    "        with open(os.path.join(RESULTS_PATH,final_file_name), \"wb\") as f:\n",
    "            pickle.dump(run_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b351580",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
