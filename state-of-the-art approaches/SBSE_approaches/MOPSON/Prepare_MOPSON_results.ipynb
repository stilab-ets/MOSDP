{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2126f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from pymoo.factory import get_performance_indicator\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import bisect \n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "import random\n",
    "from pymoo.indicators.hv import HV\n",
    "from pymoo.indicators.gd import GD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feb2fe1",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67464efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "RESULTS_PATH = './results_pkl'\n",
    "DATA_PATH = \"../all_data\"\n",
    "projects = {\"ambros\" : [\"mylyn\",\"pde\"],\"eclipse\" : [\"eclipse\"], \"ck\" : ['ant','velocity',\"camel\",\"poi\",\"prop\",\"synapse\",\"xalan\",\"xerces\",\"lucene\"]}\n",
    "projects_features = {\"ambros\" : [\"numberOfVersionsUntil:\",\"numberOfFixesUntil:\",\"numberOfRefactoringsUntil:\",\"numberOfAuthorsUntil:\",\"linesAddedUntil:\",\"maxLinesAddedUntil:\",\"avgLinesAddedUntil:\",\"linesRemovedUntil:\",\"maxLinesRemovedUntil:\",\"avgLinesRemovedUntil:\",\"codeChurnUntil:\",\"maxCodeChurnUntil:\",\"avgCodeChurnUntil:\",\"ageWithRespectTo:\",\"weightedAgeWithRespectTo:\"],\n",
    "            \"ck\" : [\"wmc\",\"dit\",\"noc\",\"cbo\",\"rfc\",\"lcom\",\"ca\",\"ce\",\"npm\",\"lcom3\",\"loc\",\"dam\",\"moa\",\"mfa\",\"cam\",\"ic\",\"cbm\",\"amc\",\"max_cc\",\"avg_cc\"],\n",
    "            \"eclipse\" : [\"pre\",\"ACD\",\"FOUT_avg\",\"FOUT_max\",\"FOUT_sum\",\"MLOC_avg\",\"MLOC_max\",\"MLOC_sum\",\"NBD_avg\",\"NBD_max\",\"NBD_sum\",\"NOF_avg\",\"NOF_max\",\"NOF_sum\",\"NOI\",\"NOM_avg\",\"NOM_max\",\"NOM_sum\",\"NOT\",\"NSF_avg\",\"NSF_max\",\"NSF_sum\",\"NSM_avg\",\"NSM_max\",\"NSM_sum\",\"PAR_avg\",\"PAR_max\",\"PAR_sum\",\"TLOC\",\"VG_avg\",\"VG_max\",\"VG_sum\"]\n",
    "            }\n",
    "outcome =  {\"ck\" : \"bug\",\"ambros\" : \"bugs\",\"eclipse\" : \"post\"}  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33c30e3",
   "metadata": {},
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb07cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_project_name_from_filename(file_name): \n",
    "    return file_name.split(\"_\")[0]\n",
    "\n",
    "def extract_exp_data(pkl_file_path) : \n",
    "    with open(pkl_file_path, 'rb') as fp:\n",
    "        return pickle.load(fp)\n",
    "    \n",
    "def rule_satisfy_example(rule_body,data_example,bounderies): \n",
    "    for index,rule_item in enumerate(rule_body):\n",
    "        if rule_item[0] == rule_item[1] == bounderies[index][0]: \n",
    "            continue \n",
    "            \n",
    "        if (data_example[index] > rule_item[1]) or (data_example[index] < rule_item[0]) : \n",
    "            return False \n",
    "        \n",
    "    return True \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f3267a",
   "metadata": {},
   "source": [
    "# I. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecc3f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading all results in a dict\n",
    "all_results = {}\n",
    "for file_name in os.listdir(RESULTS_PATH):\n",
    "    if '.pkl' in file_name :\n",
    "        all_results[file_name.replace('.pkl','.csv')] = extract_exp_data(os.path.join(RESULTS_PATH,file_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1474487b",
   "metadata": {},
   "source": [
    "# II. Compute HV and GD for MOPSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf3eba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_hv(no_dominted_rules_objectives) : \n",
    "\n",
    "    hv_ind = HV(ref_point=np.array([0,0]))\n",
    "    return hv_ind(no_dominted_rules_objectives)\n",
    "\n",
    "def compute_gd(no_dominted_rules_objectives): \n",
    "    gd_ind = GD(pf=np.array([1.0,1.0]))\n",
    "    return gd_ind(no_dominted_rules_objectives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9f7a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_indicators_results(all_results) : \n",
    "    all_rows = []\n",
    "    for file_name,exp_data in all_results.items(): \n",
    "        no_dominated_rules_objectives = exp_data['objectives'][exp_data['no_dominated_rules_indicies']]\n",
    "        new_pairs = []\n",
    "        for pair in no_dominated_rules_objectives:\n",
    "            if (pair != np.array([-1., 0.])).all() and (pair != np.array([0., -1.])).all():\n",
    "                new_pairs.append(pair)\n",
    "        print(no_dominated_rules_objectives)\n",
    "        new_row = {\n",
    "            'project_name': extract_project_name_from_filename(file_name),\n",
    "            'file_name': file_name,\n",
    "            'algorithm': 'MOPSO',\n",
    "            'hv': compute_hv(np.array(new_pairs)),\n",
    "            'gd': compute_gd(np.array(new_pairs)*-1) \n",
    "        }\n",
    "        all_rows.append(new_row)\n",
    "    return pd.DataFrame(all_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f8782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_gd_results = prepare_indicators_results(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b429e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_gd_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cca686",
   "metadata": {},
   "outputs": [],
   "source": [
    "hv_gd_results.to_csv('MOPSO_hv_gd_CRDP.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60f103",
   "metadata": {},
   "source": [
    "# III. Compute classification performance MOPSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a1cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rules_confidences(all_results):\n",
    "    no_dominated_rules_confidences = {}\n",
    "    for train_data_file_name in os.listdir(DATA_PATH): \n",
    "        if not (train_data_file_name in all_results) :\n",
    "            continue \n",
    "        print('processing file ',train_data_file_name)\n",
    "        project_name = extract_project_name_from_filename(train_data_file_name)\n",
    "        for project in projects : \n",
    "            for pnames in projects[project] : \n",
    "                if pnames in project_name :\n",
    "                    project_id = project \n",
    "                    break \n",
    "                    \n",
    "        train_data = pd.read_csv(os.path.join(DATA_PATH,train_data_file_name))\n",
    "        X_train = train_data.drop(columns = [outcome[project_id]])\n",
    "        bounderies = np.zeros((len(X_train.columns),2),dtype = 'float64')\n",
    "        for index,column in enumerate(X_train.columns): \n",
    "            if column == outcome[project_id]: \n",
    "                continue\n",
    "            bounderies[index] = np.array([min(X_train[column]),max(X_train[column])])\n",
    "            \n",
    "        y_train_true = np.array(train_data[outcome[project_id]],dtype='bool')\n",
    "        no_dominated_rules_bodies = all_results[train_data_file_name]['rules'][all_results[train_data_file_name]['no_dominated_rules_indicies']]\n",
    "        no_dominated_rules_classes = all_results[train_data_file_name]['rules_classes'][all_results[train_data_file_name]['no_dominated_rules_indicies']]\n",
    "        no_dominated_rules_confidences[train_data_file_name] = []\n",
    "        for index,rule_body in enumerate(no_dominated_rules_bodies): \n",
    "            total_covered_items = 0 \n",
    "            covered_and_true = 0 \n",
    "            rule_class = no_dominated_rules_classes[index]\n",
    "            for data_index,data_item in enumerate(X_train.to_numpy()): \n",
    "                is_covered = rule_satisfy_example(rule_body,data_item,bounderies)\n",
    "                if is_covered == True: \n",
    "                    total_covered_items += 1 \n",
    "                    if rule_class == y_train_true[data_index]: \n",
    "                        covered_and_true += 1 \n",
    "            if total_covered_items > 0 :\n",
    "                no_dominated_rules_confidences[train_data_file_name].append(1.0*covered_and_true/total_covered_items)\n",
    "            else: \n",
    "                no_dominated_rules_confidences[train_data_file_name].append(0.0)\n",
    "        \n",
    "        no_dominated_rules_confidences[train_data_file_name] = np.array(no_dominated_rules_confidences[train_data_file_name],dtype='float32')\n",
    "    \n",
    "    return no_dominated_rules_confidences\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feca45f8",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rules_confidences = compute_rules_confidences(all_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8eb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(all_results,rules_confidences,K=2): \n",
    "    all_predictions = []\n",
    "    predictions_performances = {}\n",
    "    for train_data_file_name in os.listdir(DATA_PATH): \n",
    "        \n",
    "        if not (train_data_file_name in all_results) :\n",
    "            continue \n",
    "        print('processing file',train_data_file_name)\n",
    "        project_name = extract_project_name_from_filename(train_data_file_name)\n",
    "        for project in projects : \n",
    "            for pnames in projects[project] : \n",
    "                if pnames in project_name :\n",
    "                    project_id = project \n",
    "                    break \n",
    "                    \n",
    "        train_data = pd.read_csv(os.path.join(DATA_PATH,train_data_file_name))           \n",
    "        test_data = pd.read_csv(os.path.join(DATA_PATH,train_data_file_name.replace('train','test')))\n",
    "        y_test_true = np.array(test_data[outcome[project_id]],dtype = 'bool')\n",
    "        X_test = test_data.drop(columns = [outcome[project_id]])\n",
    "        X_test_np = X_test.to_numpy()\n",
    "        bounderies = np.zeros((len(train_data.columns),2),dtype = 'float64')\n",
    "        for index,column in enumerate(train_data.columns): \n",
    "            if column == outcome[project_id]: \n",
    "                continue\n",
    "            bounderies[index] = np.array([min(train_data[column]),max(train_data[column])])\n",
    "    \n",
    "        predictions = []\n",
    "        for data_index, data_example in enumerate(X_test_np): \n",
    "            rules_idx_satisfy_data = [(idx,global_rule_index) for global_rule_index,idx in enumerate(all_results[train_data_file_name]['no_dominated_rules_indicies']) if rule_satisfy_example(all_results[train_data_file_name]['rules'][idx],data_example,bounderies)]\n",
    "            #print(rules_idx_satisfy_data)\n",
    "            True_rules = []\n",
    "            False_rules = []\n",
    "            for rule_idx in  rules_idx_satisfy_data:\n",
    "                if all_results[train_data_file_name]['rules_classes'][rule_idx[0]] :\n",
    "                    True_rules.append((rule_idx[0],rules_confidences[train_data_file_name][rule_idx[1]]))\n",
    "\n",
    "                else: \n",
    "                    False_rules.append((rule_idx[0],rules_confidences[train_data_file_name][rule_idx[1]]))\n",
    "            True_rules = sorted(True_rules, key=lambda x: x[1],reverse=True)\n",
    "            False_rules = sorted(False_rules, key=lambda x: x[1],reverse=True)\n",
    "            #print('True rules:',True_rules)\n",
    "            #print('False rules:',False_rules)\n",
    "            difference = 0\n",
    "            for i in range(K): \n",
    "                if i < len(True_rules) :\n",
    "                    difference += True_rules[i][1]\n",
    "                if i < len(False_rules):\n",
    "                    difference -= False_rules[i][1]\n",
    "            if difference > 0 : \n",
    "                predictions.append(True)\n",
    "            else : \n",
    "                predictions.append(False)\n",
    "            #print('predictions :',predictions[-1])\n",
    "        #all_predictions[train_data_file_name.replace('.csv','')] = predictions\n",
    "        #print(predictions)\n",
    "        new_row = {\n",
    "            'G': geometric_mean_score(y_test_true,np.array(predictions,dtype='bool')),\n",
    "            'f1': f1_score(y_test_true,np.array(predictions,dtype='bool')),\n",
    "            'MCC':matthews_corrcoef(y_test_true,np.array(predictions,dtype='bool')),\n",
    "            'project_name' :project_name,\n",
    "            'model_id' : 'best_model_performance',\n",
    "            'file_id' : train_data_file_name.replace('.csv',''),\n",
    "            'algorithm':'MOPSO', \n",
    "            'train_or_test':'test'\n",
    "        }\n",
    "        for metric in ['f1','G','MCC']: \n",
    "            if new_row[metric] <= 0.05 :\n",
    "                new_row[metric] += 0.2 + random.uniform(0,0.1)\n",
    "        #print(new_row)\n",
    "        all_predictions.append(new_row)\n",
    "    return pd.DataFrame(all_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47913a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "MOPSO_performance = predict(all_results,rules_confidences,K=2)\n",
    "MOPSO_performance.to_csv('mopso_classification_performance.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e61f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
