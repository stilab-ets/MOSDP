{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import pandas as pd \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from imblearn.metrics import geometric_mean_score\n",
    "from imblearn.metrics import specificity_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from  tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepRandomForest: \n",
    "    def __init__(self,configs = {}) -> None:\n",
    "        self.default_configs = {\n",
    "            'estimator' : RandomForestClassifier(n_estimators=500, max_features='sqrt',n_jobs=-1),\n",
    "            'param_M' : 4,\n",
    "            'standarize' : False,\n",
    "            'param_tolerance' : 0.0,\n",
    "            'maximum_layer_number' : np.inf,\n",
    "            'validation_size' : 0.2\n",
    "        }\n",
    "        self.actual_configs = copy.deepcopy(self.default_configs)\n",
    "        for param_name, param_value in configs : \n",
    "            self.actual_configs[param_name] = param_value\n",
    "    \n",
    "    def fit(self,X,y) :\n",
    "        self.original_X = X \n",
    "        self.final_X = np.array(X)\n",
    "        self.original_y = y \n",
    "        if self.actual_configs['standarize'] : \n",
    "            scaler = StandardScaler()\n",
    "            self.final_X = scaler.fit_transform(X)\n",
    "            self.actual_configs['scaler'] = scaler\n",
    "\n",
    "        X_train, X_val, y_train, y_val  = train_test_split(self.final_X, self.original_y, \n",
    "                                                            test_size=self.actual_configs['validation_size'])\n",
    "       \n",
    "        \n",
    "        self.layers = [self.create_layer()]\n",
    "        self.fit_layer(self.layers[0],X_train,y_train)\n",
    "        layer_predictions_train = self.layer_predict(self.layers[0],X_train)\n",
    "        layer_proba_prediction_train = self.compute_layer_prediction(self.layers[0], X_train)\n",
    "        layer_predictions_val = self.layer_predict(self.layers[0],X_val)\n",
    "        layer_proba_prediction_val = self.compute_layer_prediction(self.layers[0], X_val)\n",
    "        ref_accuracy = self.cascade_accuracy(layer_predictions_val,y_val)\n",
    "        n_layers = 1 \n",
    "        max_acc_reached = False \n",
    "        # error definition is missing \n",
    "        while n_layers < self.actual_configs['maximum_layer_number']: \n",
    "            X_train = update_X(X_train, layer_proba_prediction_train)\n",
    "            X_val = update_X(X_val, layer_proba_prediction_val)\n",
    "            new_layer = self.create_layer()\n",
    "            self.fit_layer(new_layer,X_train,y_train)\n",
    "            \n",
    "\n",
    "            layer_predictions_val = self.layer_predict(new_layer,X_val)\n",
    "            \n",
    "            layer_proba_prediction_val = self.compute_layer_prediction(new_layer,X_val)\n",
    "\n",
    "            new_ref_accuracy = self.cascade_accuracy(layer_predictions_val,y_val)\n",
    "            if new_ref_accuracy <=  ref_accuracy:\n",
    "                max_acc_reached = True \n",
    "                break\n",
    "            else : \n",
    "                ref_accuracy = new_ref_accuracy \n",
    "                n_layers += 1 \n",
    "                self.layers.append(new_layer)\n",
    "                layer_proba_prediction_train = self.compute_layer_prediction(new_layer,X_train)\n",
    "    def create_layer(self) : \n",
    "        return [copy.deepcopy(self.actual_configs['estimator']) for _ in range(self.actual_configs['param_M'])]\n",
    "    \n",
    "    def fit_layer(self,layer,X,y) : \n",
    "        for estimator in layer : \n",
    "            estimator.fit(X,y)\n",
    "    \n",
    "    def predict(self,X) : \n",
    "        probs = self.predict_proba(X) \n",
    "        return probs.argmax(axis=1)\n",
    "\n",
    "    def predict_proba(self,X) : \n",
    "        X_for_prediction = np.array(X)\n",
    "        for layer_index in range(len(self.layers)-1)  : \n",
    "            layer_predictions = self.compute_layer_prediction(self.layers[layer_index],X_for_prediction)\n",
    "            X_for_prediction = update_X(X_for_prediction,layer_predictions)\n",
    "        last_layer = self.layers[-1]\n",
    "        #last_layer_predictions_probs = self.compute_layer_prediction(self.layers[layer_index],X_for_prediction).reshape(len(X),self.actual_configs['param_M'],-1)\n",
    "        return self.layer_predict_proba(last_layer,X_for_prediction)\n",
    "\n",
    "        \n",
    "\n",
    "    def compute_layer_prediction(self,layer,X) :\n",
    "        layer_perdictions = []\n",
    "        for estimator_index,estimator in enumerate(layer) : \n",
    "            estimator_proba = estimator.predict_proba(X)\n",
    "            layer_perdictions.append(estimator_proba)\n",
    "        return np.concatenate((layer_perdictions),axis=1)\n",
    "\n",
    "    \n",
    "    def layer_predict(self,layer,X) : \n",
    "        probs = self.layer_predict_proba(layer,X)\n",
    "        return probs.argmax(axis=1)\n",
    "\n",
    "    def layer_predict_proba(self,layer,X) : \n",
    "        return self.compute_layer_prediction(layer,X).reshape(len(X),self.actual_configs['param_M'],-1).mean(axis = 1)\n",
    "    \n",
    "    \n",
    "    def cascade_accuracy(self,cascade_preds,y_test) : \n",
    "        return accuracy_score(y_true=y_test,y_pred=cascade_preds)\n",
    "        \n",
    "def update_X(X,predictions) : \n",
    "        result = copy.deepcopy(X)\n",
    "        return np.concatenate((result,predictions),axis=1)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main\n",
    "DATA_PATH = './CRDP_data'\n",
    "RESULTS_PATH = \"./Results\"\n",
    "os.makedirs(RESULTS_PATH,exist_ok=True)\n",
    "datasets = {\n",
    "    'ambros' : {\n",
    "        \"features\" : [\"numberOfVersionsUntil:\",\"numberOfFixesUntil:\",\"numberOfRefactoringsUntil:\",\"numberOfAuthorsUntil:\",\n",
    "                      \"linesAddedUntil:\",\"maxLinesAddedUntil:\",\"avgLinesAddedUntil:\",\"linesRemovedUntil:\",\"maxLinesRemovedUntil:\",\n",
    "                      \"avgLinesRemovedUntil:\",\"codeChurnUntil:\",\"maxCodeChurnUntil:\",\"avgCodeChurnUntil:\",\"ageWithRespectTo:\",\n",
    "                      \"weightedAgeWithRespectTo:\"],\n",
    "        \"outcome\" : 'bugs'\n",
    "    },\n",
    "        \"ck\" : {\n",
    "        'features' :['wmc', 'dit', 'noc', 'cbo', 'rfc', 'lcom',\n",
    "       'ca', 'ce', 'npm', 'lcom3', 'loc', 'dam', 'moa', 'mfa', 'cam', 'ic',\n",
    "       'cbm', 'amc', 'max_cc', 'avg_cc'],\n",
    "       'outcome' : 'bug'\n",
    "    },\n",
    "    \n",
    "\n",
    "    'eclipse' : {\n",
    "        'features' :  [\"pre\",\"ACD\",\"FOUT_avg\",\"FOUT_max\",\"FOUT_sum\",\"MLOC_avg\",\"MLOC_max\",\"MLOC_sum\",\"NBD_avg\",\n",
    "                       \"NBD_max\",\"NBD_sum\",\"NOF_avg\",\"NOF_max\",\"NOF_sum\",\"NOI\",\"NOM_avg\",\"NOM_max\",\"NOM_sum\",\"NOT\"\n",
    "                       ,\"NSF_avg\",\"NSF_max\",\"NSF_sum\",\"NSM_avg\",\"NSM_max\",\"NSM_sum\",\"PAR_avg\",\"PAR_max\",\"PAR_sum\",\n",
    "                       \"TLOC\",\"VG_avg\",\"VG_max\",\"VG_sum\"],\n",
    "        'outcome' : 'post'\n",
    "    }\n",
    "}\n",
    "for APPLY_SMOTE in [True, False]:\n",
    "    all_results = []\n",
    "    for dataset, dataset_data in datasets.items() : \n",
    "        \n",
    "        dataset_path = os.path.join(DATA_PATH,dataset)\n",
    "        if not(os.path.exists(dataset_path)): \n",
    "            continue \n",
    "        print('working on dataset:',dataset)\n",
    "        all_files = [file for file in os.listdir(dataset_path)]\n",
    "        pbar = tqdm(total= len(all_files)/2)\n",
    "        features = dataset_data['features']\n",
    "        outcome = dataset_data['outcome']\n",
    "        dataset_results = []\n",
    "        for file in os.listdir(dataset_path):\n",
    "            if not ('train' in file) : \n",
    "                continue\n",
    "\n",
    "            train_df = pd.read_csv(os.path.join(dataset_path,file))\n",
    "            test_df =  pd.read_csv(os.path.join(dataset_path,file.replace('train','test')))\n",
    "            X_train, y_train = train_df[features],  train_df[outcome]\n",
    "            X_test, y_test = test_df[features],  test_df[outcome]\n",
    "            if APPLY_SMOTE  : \n",
    "                sm = SMOTE(random_state=43)\n",
    "                try:\n",
    "                    X_train, y_train = sm.fit_resample(X=train_df.loc[:,features].values, y=train_df.loc[:,outcome].values)\n",
    "                except: \n",
    "                    print(file)\n",
    "                    print(\"resampling problem\")\n",
    "                    pbar.update(1)\n",
    "                    continue\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "            dfdp_model =  DeepRandomForest()\n",
    "            try:\n",
    "                dfdp_model.fit(X_train,y_train)\n",
    "            except: \n",
    "                print(file)\n",
    "                print(\"fitting problem\")\n",
    "                pbar.update(1)\n",
    "                continue\n",
    "            train_predictions = dfdp_model.predict(X_train)\n",
    "            test_predictions = dfdp_model.predict(X_test)\n",
    "            project_name = file.split('_')[0]\n",
    "            train_row = {\n",
    "                \"file_id\" : file,\n",
    "                \"algorithm\" : \"DFDP\",\n",
    "                'model_id' : 'best_performance_model',\n",
    "                'train_or_test': 'train'\n",
    "            }\n",
    "            train_row.update({\n",
    "                \"f1\" :f1_score(y_true=y_train,y_pred=train_predictions),\n",
    "                'MCC' : matthews_corrcoef(y_true=y_train,y_pred=train_predictions),\n",
    "                \"G\": geometric_mean_score(y_true=y_train,y_pred=train_predictions),\n",
    "                \"precision\": precision_score(y_true=y_train,y_pred=train_predictions),\n",
    "                \"tpr\" : recall_score(y_true=y_train,y_pred=train_predictions),\n",
    "                \"tnr\" : specificity_score(y_true=y_train,y_pred=train_predictions)\n",
    "            })\n",
    "            test_row = copy.deepcopy(train_row)\n",
    "            test_row['train_or_test'] = 'test'\n",
    "            test_row.update({\n",
    "                \"f1\" :f1_score(y_true=y_test,y_pred=test_predictions),\n",
    "                'MCC' : matthews_corrcoef(y_true=y_test,y_pred=test_predictions),\n",
    "                \"G\": geometric_mean_score(y_true=y_test,y_pred=test_predictions),\n",
    "                \"precision\": precision_score(y_true=y_test,y_pred=test_predictions),\n",
    "                \"tpr\" : recall_score(y_true=y_test,y_pred=test_predictions),\n",
    "                \"tnr\" : specificity_score(y_true=y_test,y_pred=test_predictions)\n",
    "            })\n",
    "            dataset_results.append(train_row)\n",
    "            dataset_results.append(test_row)\n",
    "            pbar.update(1)\n",
    "        all_results += dataset_results\n",
    "        dataset_results = pd.DataFrame(all_results)\n",
    "        if APPLY_SMOTE  : \n",
    "            dataset_results.to_csv(os.path.join(RESULTS_PATH,f\"DFDP_CRDP_SMOTE_{dataset}.csv\"),index=False)\n",
    "        else:\n",
    "            dataset_results.to_csv(os.path.join(RESULTS_PATH,f\"DFDP_CRDP_{dataset}.csv\"),index=False)\n",
    "    all_results = pd.DataFrame(all_results)\n",
    "    if APPLY_SMOTE : \n",
    "        all_results.to_csv(\"DFDP_CRDP_SMOTE_all.csv\",index=False)\n",
    "    else :\n",
    "        all_results.to_csv(\"DFDP_CRDP_all.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c615651ec58bb9f6d2e3aeabec9f1b3f3ed5bc61b7a4d28e8d8d4e50b8b176fe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
